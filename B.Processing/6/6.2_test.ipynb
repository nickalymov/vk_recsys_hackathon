{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5de76b33d4673490",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T15:06:16.345413200Z",
     "start_time": "2024-12-03T15:06:16.327757700Z"
    }
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'data_folder': 'C:/Users/Николай/PycharmProjects/VKRecSys/data/',\n",
    "    'models_folder': 'C:/Users/Николай/PycharmProjects/VKRecSys/B.Processing/Модели/',\n",
    "    'results_folder': 'C:/Users/Николай/PycharmProjects/VKRecSys/C.Results/',\n",
    "\n",
    "    'test_path': 'test_pairs.csv',  \n",
    "    'model_path': '6.2.pth', \n",
    "    'output_path': '6.2_predictions.csv',\n",
    "    'items_meta_path' : 'items_meta.parquet',\n",
    "    'users_meta_path' : 'users_meta.parquet',\n",
    "\n",
    "    'user_emb_size' : 512, # 183404\n",
    "    'item_emb_size' : 512, # 337727\n",
    "    'source_emb_size' : 128, # 19613\n",
    "    'age_emb_size' : 4, # 43\n",
    "    'duration_emb_size' : 16, # ~175   \n",
    "    'gender_emb_size' : 4, # 2\n",
    "    'torch_precision' : 40, # number of decimal places for printing numbers\n",
    "\n",
    "    'DEVICE': 'cuda',\n",
    "    'SEED': 42,\n",
    "    'BATCH_SIZE': 16384,\n",
    "    'output_dim': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T15:06:18.001101700Z",
     "start_time": "2024-12-03T15:06:16.346412700Z"
    }
   },
   "id": "2af2aecfc6847170",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device(CONFIG['DEVICE'] if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_printoptions(precision=CONFIG['torch_precision'])\n",
    "\n",
    "torch.manual_seed(CONFIG['SEED'])\n",
    "torch.cuda.manual_seed_all(CONFIG['SEED'])\n",
    "np.random.seed(CONFIG['SEED'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T15:06:18.666639300Z",
     "start_time": "2024-12-03T15:06:18.648639400Z"
    }
   },
   "id": "404582cb35e90296",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test = pd.read_csv(f\"{CONFIG['data_folder']}{CONFIG['test_path']}\")\n",
    "\n",
    "items_meta = pd.read_parquet(f\"{CONFIG['data_folder']}{CONFIG['items_meta_path']}\", engine='pyarrow')\n",
    "items_meta['duration'] = items_meta['duration'] - 5\n",
    "items_meta['item_id'] = items_meta['item_id'].astype('category')\n",
    "items_meta['source_id'] = items_meta['source_id'].astype('category')\n",
    "items_meta['duration'] = items_meta['duration'].astype('category')\n",
    "items_meta.set_index('item_id', inplace=True)\n",
    "\n",
    "users_meta = pd.read_parquet(f\"{CONFIG['data_folder']}{CONFIG['users_meta_path']}\", engine='pyarrow')\n",
    "users_meta['age'] = users_meta['age'] - 18\n",
    "users_meta['gender'] = users_meta['gender'].replace({1:0, 2:1})\n",
    "users_meta['user_id'] = users_meta['user_id'].astype('category')\n",
    "users_meta['gender'] = users_meta['gender'].astype('category')\n",
    "users_meta['age'] = users_meta['age'].astype('category')\n",
    "users_meta.set_index('user_id', inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T15:06:19.251637200Z",
     "start_time": "2024-12-03T15:06:18.670640900Z"
    }
   },
   "id": "75461867fc71ecda",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class AFMModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 num_users=users_meta.index.nunique(), \n",
    "                 num_items=items_meta.index.nunique(), \n",
    "                 num_sources=items_meta['source_id'].nunique(),\n",
    "                 num_ages=users_meta['age'].nunique(),\n",
    "                 num_durations=items_meta['duration'].nunique(),\n",
    "                 num_genders=users_meta['gender'].nunique(), \n",
    "                 output_dim=CONFIG['output_dim']): \n",
    "        super(AFMModel, self).__init__()\n",
    "        \n",
    "        # Embedding layers for each feature\n",
    "        self.user_embedding = nn.Embedding(num_users, CONFIG['user_emb_size'])\n",
    "        self.item_embedding = nn.Embedding(num_items, CONFIG['item_emb_size'])\n",
    "        self.source_embedding = nn.Embedding(num_sources, CONFIG['source_emb_size'])\n",
    "        self.age_embedding = nn.Embedding(num_ages, CONFIG['age_emb_size'])\n",
    "        self.duration_embedding = nn.Embedding(num_durations, CONFIG['duration_emb_size'])\n",
    "        self.gender_embedding = nn.Embedding(num_genders, CONFIG['gender_emb_size'])\n",
    "        \n",
    "        # Transformations for embeddings\n",
    "        self.user_emb_transform = nn.Linear(CONFIG['user_emb_size'], 32)\n",
    "        self.item_emb_transform = nn.Linear(CONFIG['item_emb_size'], 32)\n",
    "        self.source_emb_transform = nn.Linear(CONFIG['source_emb_size'], 32)\n",
    "        self.age_emb_transform = nn.Linear(CONFIG['age_emb_size'], 32)\n",
    "        self.duration_emb_transform = nn.Linear(CONFIG['duration_emb_size'], 32)\n",
    "        self.gender_emb_transform = nn.Linear(CONFIG['gender_emb_size'], 32)\n",
    "        \n",
    "        # Attention layer components\n",
    "        self.attention_network = nn.Sequential(\n",
    "            nn.Linear(64, 32),  # 64 comes from the concatenation of two embedding sizes\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(32, output_dim)  # Output layer\n",
    "    \n",
    "    def pairwise_interaction(self, x1, x2):\n",
    "        return x1 * x2  # Element-wise multiplication for feature interaction\n",
    "    \n",
    "    def compute_attention(self, emb1, emb2):\n",
    "        # Concatenate embeddings of the pair of features and pass through attention network\n",
    "        combined_emb = torch.cat([emb1, emb2], dim=-1)  # Concatenate embeddings (dim=-1 is feature dimension)\n",
    "        attention_score = self.attention_network(combined_emb)\n",
    "        attention_score = torch.sigmoid(attention_score)  # Use sigmoid for attention score\n",
    "        return attention_score\n",
    "    \n",
    "    def forward(self, user_ids, item_ids, source_ids, age_ids, duration_ids, gender_ids, embeddings):\n",
    "        # Obtain embeddings for all features\n",
    "        user_emb = self.user_embedding(user_ids)\n",
    "        item_emb = self.item_embedding(item_ids)\n",
    "        source_emb = self.source_embedding(source_ids)\n",
    "        age_emb = self.age_embedding(age_ids)\n",
    "        duration_emb = self.duration_embedding(duration_ids)\n",
    "        gender_emb = self.gender_embedding(gender_ids)\n",
    "        \n",
    "        # Apply linear transformations to embeddings\n",
    "        user_emb = self.user_emb_transform(user_emb)\n",
    "        item_emb = self.item_emb_transform(item_emb)\n",
    "        source_emb = self.source_emb_transform(source_emb)\n",
    "        age_emb = self.age_emb_transform(age_emb)\n",
    "        duration_emb = self.duration_emb_transform(duration_emb)\n",
    "        gender_emb = self.gender_emb_transform(gender_emb)\n",
    "        \n",
    "        # Create dictionary of all embeddings\n",
    "        embeddings_dict = {\n",
    "            'user': user_emb,\n",
    "            'item': item_emb,\n",
    "            'source': source_emb,\n",
    "            'age': age_emb,\n",
    "            'duration': duration_emb,\n",
    "            'gender': gender_emb,\n",
    "            'embeddings': embeddings\n",
    "        }\n",
    "        \n",
    "        interactions = []\n",
    "        attention_scores = []\n",
    "        \n",
    "        # Compute pairwise interactions and their attention scores\n",
    "        for key1, emb1 in embeddings_dict.items():\n",
    "            for key2, emb2 in embeddings_dict.items():\n",
    "                if key1 < key2:  # Avoid double counting pairs (e.g., user-item and item-user)\n",
    "                    interaction = self.pairwise_interaction(emb1, emb2)\n",
    "                    attention_score = self.compute_attention(emb1, emb2)\n",
    "                    interactions.append(interaction * attention_score)  # Weight the interaction by attention score\n",
    "                    attention_scores.append(attention_score)  # Store the attention scores\n",
    "        \n",
    "        # Combine all interactions into a single tensor\n",
    "        interactions = torch.stack(interactions, dim=1)  # Shape: [batch_size, num_interactions]\n",
    "        attention_scores = torch.stack(attention_scores, dim=1)  # Shape: [batch_size, num_interactions]\n",
    "        \n",
    "        # Sum over all interactions with their respective attention scores\n",
    "        weighted_interactions = torch.sum(interactions * attention_scores, dim=1)\n",
    "        \n",
    "        # Pass through output layer to get the final prediction\n",
    "        output = self.output_layer(weighted_interactions)\n",
    "        \n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T15:06:19.285656100Z",
     "start_time": "2024-12-03T15:06:19.259637300Z"
    }
   },
   "id": "a8659ecba5b8dcca",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "input_dim = (CONFIG['user_emb_size'] + \n",
    "             CONFIG['item_emb_size'] + \n",
    "             CONFIG['source_emb_size'] + \n",
    "             CONFIG['age_emb_size'] +\n",
    "             CONFIG['duration_emb_size'] + \n",
    "             CONFIG['gender_emb_size'] + \n",
    "             32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T15:06:19.299654100Z",
     "start_time": "2024-12-03T15:06:19.286656400Z"
    }
   },
   "id": "f37b42fb9a81929b",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Николай\\AppData\\Local\\Temp\\ipykernel_15348\\2852703818.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"{CONFIG['models_folder']}{CONFIG['model_path']}\")['model_state_dict'])\n"
     ]
    },
    {
     "data": {
      "text/plain": "AFMModel(\n  (user_embedding): Embedding(183404, 512)\n  (item_embedding): Embedding(337727, 512)\n  (source_embedding): Embedding(19613, 128)\n  (age_embedding): Embedding(43, 4)\n  (duration_embedding): Embedding(176, 16)\n  (gender_embedding): Embedding(2, 4)\n  (user_emb_transform): Linear(in_features=512, out_features=32, bias=True)\n  (item_emb_transform): Linear(in_features=512, out_features=32, bias=True)\n  (source_emb_transform): Linear(in_features=128, out_features=32, bias=True)\n  (age_emb_transform): Linear(in_features=4, out_features=32, bias=True)\n  (duration_emb_transform): Linear(in_features=16, out_features=32, bias=True)\n  (gender_emb_transform): Linear(in_features=4, out_features=32, bias=True)\n  (attention_network): Sequential(\n    (0): Linear(in_features=64, out_features=32, bias=True)\n    (1): GELU(approximate='none')\n    (2): Linear(in_features=32, out_features=1, bias=True)\n  )\n  (output_layer): Linear(in_features=32, out_features=3, bias=True)\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AFMModel(input_dim).to(device)\n",
    "model.load_state_dict(torch.load(f\"{CONFIG['models_folder']}{CONFIG['model_path']}\")['model_state_dict'])\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T15:06:22.206465Z",
     "start_time": "2024-12-03T15:06:19.303654300Z"
    }
   },
   "id": "92808a74f0ca9b8",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 102/102 [00:08<00:00, 12.05batch/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "num_samples = len(test)\n",
    "num_batches = (num_samples + CONFIG['BATCH_SIZE'] - 1) // CONFIG['BATCH_SIZE']\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx in tqdm(range(num_batches), desc=\"Predicting\", unit=\"batch\"):\n",
    "        start_idx = batch_idx * CONFIG['BATCH_SIZE']\n",
    "        end_idx = min(start_idx + CONFIG['BATCH_SIZE'], num_samples)\n",
    "        batch_main = test.iloc[start_idx:end_idx]\n",
    "        batch_user_ids = batch_main['user_id'].values\n",
    "        batch_item_ids = batch_main['item_id'].values\n",
    "        \n",
    "        batch_users_meta = users_meta.loc[batch_user_ids]\n",
    "        batch_items_meta = items_meta.loc[batch_item_ids]\n",
    "        \n",
    "        batch_user_values = torch.tensor(batch_user_ids, dtype=torch.long, device=device)\n",
    "        batch_item_values = torch.tensor(batch_item_ids, dtype=torch.long, device=device)\n",
    "        \n",
    "        batch_gender_values = torch.tensor(batch_users_meta['gender'].values, dtype=torch.long, device=device)\n",
    "        batch_age_values = torch.tensor(batch_users_meta['age'].values, dtype=torch.long, device=device)\n",
    "        batch_source_values = torch.tensor(batch_items_meta['source_id'].values, dtype=torch.long, device=device)\n",
    "        batch_duration_values = torch.tensor(batch_items_meta['duration'].values, dtype=torch.long, device=device)\n",
    "\n",
    "        embeddings = torch.tensor(np.stack(batch_items_meta['embeddings'].values), device=device, dtype=torch.float32)\n",
    "\n",
    "        outputs = model(batch_user_values, batch_item_values, batch_source_values, batch_age_values, batch_duration_values, batch_gender_values, embeddings)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "\n",
    "        # Взвешенные предсказания\n",
    "        class_weights = torch.tensor([0, 1, 2], device=probabilities.device, dtype=probabilities.dtype)\n",
    "        weighted_predictions = torch.sum(probabilities * class_weights, dim=1).cpu().numpy()\n",
    "\n",
    "        predictions.extend(weighted_predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T15:06:30.690701300Z",
     "start_time": "2024-12-03T15:06:22.207465600Z"
    }
   },
   "id": "d3a3fa8ef64050c9",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to f'C:/Users/Николай/PycharmProjects/VKRecSys/C.Results/6.2_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "# Добавление предсказанных значений в DataFrame\n",
    "test['predict'] = predictions\n",
    "\n",
    "test.to_csv(f\"{CONFIG['results_folder']}{CONFIG['output_path']}\", index=False)\n",
    "print(f\"Predictions saved to f'{CONFIG['results_folder']}{CONFIG['output_path']}'\")"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-03T15:06:34.190951200Z",
     "start_time": "2024-12-03T15:06:30.693700900Z"
    }
   },
   "id": "initial_id",
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
